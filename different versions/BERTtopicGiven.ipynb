{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Necessary Libraries\n",
    "\n",
    "Make sure  Microsoft Visual C++ is installed on your pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bertopic \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Load Your Data\n",
    "\n",
    "Load the articles from your CSV file (e.g. studies_lobke.csv) using pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df= pd.read_csv('studies.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Your Text Data\n",
    "We clean up the text\n",
    "- Remove special characters (only letters)\n",
    "- Convert to lower case\n",
    "- Remove stop words\n",
    "- Remove words of only one or 2 letters ('a', 'I', at,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import download\n",
    "\n",
    "# Ensure WordNet and stopwords are downloaded\n",
    "download('wordnet')\n",
    "download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Customize stopword list as needed\n",
    "stop_words.update(['study'])\n",
    "\n",
    "# Minimum word size threshold\n",
    "minWordSize = 3\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function that outputs a cleaned sentence\n",
    "def preprocess_text(text):\n",
    "    # Normalize Unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Remove text between parentheses and non-alphabetic characters\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Convert text to lowercase and split into words, lemmatizing each word\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in text.split()]\n",
    "    \n",
    "    # Filter out stopwords and short words\n",
    "    words = [w for w in lemmatized_words if w not in stop_words and len(w) >= minWordSize]\n",
    "    \n",
    "    # Join the cleaned words back into a single string\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the preprocessing function to the 'text' column, producing clean sentences\n",
    "df['text_clean'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df.head()\n",
    "\n",
    "\n",
    "# Extract the articles text\n",
    "#documents = data['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Initialize and Fit BERTopic\n",
    "The good thing with BERTopic is that is does most of the work automatically (Meaning, I do not need to bore you to death with details about how it works behind te scenes.)\n",
    "\n",
    "We need to do 3 things\n",
    "1. Initialize BERTopic model\n",
    "2. 'Fit' the model -> this  means: run the model, as you would run a simple linear regression\n",
    "3. Look at the topics via \n",
    "\n",
    "To get started, let's just use the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize BERTopic model\n",
    "topic_model = BERTopic(calculate_probabilities=True)\n",
    "\n",
    "# Fit the model with preprocessed text sentences\n",
    "topics, probabilities = topic_model.fit_transform(df['text_clean'])\n",
    "\n",
    "# View and inspect topics\n",
    "topic_model.get_topic_info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get two, quite intuitive topics\n",
    "1. mixed-aged classes \n",
    "2. the quality of care. \n",
    "\n",
    "The model has done a good job of identifying these topics, but by default, BERTopic tries to group documents into fewer topics. If you want more refined topics, you can increase the number of topics by tweaking the min_topic_size or nr_topics parameters.\n",
    "\n",
    "+ min_topic_size: = the minimum number of documents in a topic (default is 10).\n",
    "    + too high =  fewer topics  \n",
    "+ nr_topics: the desired number of topics you want.\n",
    "\n",
    "There are other parameters, but I would not recommend that you mess around with these, unless you really do a deep dive in the statistical properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERTopic model\n",
    "topic_model = BERTopic(calculate_probabilities=True, min_topic_size=5, nr_topics=10)\n",
    "\n",
    "# Fit the model with preprocessed text sentences\n",
    "topics, probabilities = topic_model.fit_transform(df['text_clean'])\n",
    "\n",
    "# View and inspect topics\n",
    "topic_model.get_topic_info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Visualize Topics\n",
    "We can call .visualize_topics to create a 2D representation of the topics. The  graph is a plotly interactive graph which can be converted to HTML:\n",
    "\n",
    "Note: If you get the error 'ValueError: Mime type rendering requires nbformat>=4.2.0 but it is not installed', go to terminal and type 'pip install --upgrade nbformat  ' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topics with an interactive plot\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the slider to select the topic which then lights up red. If you hover over a topic, then general information is given about the topic, including the size of the topic and its corresponding words.\n",
    "\n",
    "We can also ask for a representation of the corresponding words for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visualize Topic HierarchyÂ¶\n",
    "The topics that were created can be hierarchically reduced. In order to understand the potential hierarchical structure of the topics, we can use scipy.cluster.hierarchy to create clusters and visualize how they relate to one another. We can also see what happens to the topic representations when merging topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(df['text_clean'])\n",
    "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you hover over the black circles, you will see the topic representation at that level of the hierarchy. These representations help you understand the effect of merging certain topics. Some might be logical to merge whilst others might not. Moreover, we can now see which sub-topics can be found within certain larger themes.\n",
    "\n",
    "You can also print a text-version of the topic representation at the different levels (a bit less pretty, but maybe easier to read.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = topic_model.get_topic_tree(hierarchical_topics)\n",
    "print(tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Visualize documents\n",
    "\n",
    "We can visualize the documents (=texts) inside the topics to see if they were assigned correctly or whether they make sense. To do so, we can use the topic_model.visualize_documents() function. This function recalculates the document embeddings and reduces them to 2-dimensional space for easier visualization purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_documents(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you hover over a point, you can see which text it is. The color tells you to which topic it belongs. While this is very pretty, it might be useful to be able to just open an excel-file or csv, which contains the original text, with the assigned topic, including the topic words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add topics and probabilities to the original DataFrame\n",
    "df[\"topic_number\"] = np.argmax(probabilities, axis=1)\n",
    "\n",
    "# Also extract the topic names and assign them to the DataFrame\n",
    "info = topic_model.get_topic_info()\n",
    "topic_names = info['Representation']\n",
    "\n",
    "df['topic_name'] = df['topic_number'].map(topic_names)\n",
    "\n",
    "# Save the updated DataFrame to a CSV\n",
    "\n",
    "df['topic_name'] = df['topic_number'].map(topic_names)\n",
    "\n",
    "# Save to a new CSV file\n",
    "df.to_csv(\"studies_lobke_with_topics.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the topic distribution per document = the probability that the text belongs to each topic (if a topic is not included in the graph, the probability is 0). Eg, the topic distribution for the sixth document:(!python starts counting at 0, so 6th =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_distribution(probabilities[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Topics per full article\n",
    "\n",
    "We extract the number of times a topic is assigned within the full articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the count of times each topic is chosen within each article\n",
    "article_topic_counts = df.groupby('File')['topic_number'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "# Rename columns to 'Topic X'\n",
    "article_topic_counts.columns = [f'Topic {i}' for i in article_topic_counts.columns]\n",
    "\n",
    "# Display the table\n",
    "print(article_topic_counts)\n",
    "\n",
    "# Plot the distribution for each article\n",
    "article_topic_counts.plot(kind='bar', stacked=True, figsize=(15, 7))\n",
    "plt.title('Topic Distribution per Article (Count)')\n",
    "plt.xlabel('Article')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Topics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also do the same, but with proportions in stead of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the proportion of times each topic is chosen within each article\n",
    "article_topic_proportions = df.groupby('File')['topic_number'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "\n",
    "# Rename columns to 'Topic X'\n",
    "article_topic_proportions.columns = [f'Topic {i}' for i in article_topic_proportions.columns]\n",
    "\n",
    "# Display the table\n",
    "print(article_topic_proportions)\n",
    "\n",
    "# Plot the distribution for each article\n",
    "article_topic_proportions.plot(kind='bar', stacked=True, figsize=(15, 7))\n",
    "plt.title('Topic Distribution per Article (Proportion)')\n",
    "plt.xlabel('Article')\n",
    "plt.ylabel('Proportion')\n",
    "plt.legend(title='Topics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
