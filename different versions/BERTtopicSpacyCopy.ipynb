{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Necessary Libraries\n",
    "\n",
    "Make sure  Microsoft Visual C++ is installed on your pc\n",
    "\n",
    "Extracting text from pdf and converting to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Before anything else, Read the README.md file\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Any, Generator, Tuple, Union, Set\n",
    "\n",
    "# Third-Party Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import fitz\n",
    "from langdetect import detect, LangDetectException\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import plotly.io as pio\n",
    "from bertopic import BERTopic\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# spaCy Stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stopwords\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stopwords\n",
    "from spacy.lang.nl.stop_words import STOP_WORDS as nl_stopwords\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../configs/config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Constants\n",
    "PDF_DIRECTORY = config[\"test_pdf_directory\"]\n",
    "CLEANED_CSV = config[\"test_cleaned_csv\"]\n",
    "TOPIC_CSV = config[\"test_topics_csv\"]\n",
    "PDF_VISUAL_PATH = config[\"pdf_visual_path\"]\n",
    "\n",
    "COLUMN_FILENAME = \"filename\"\n",
    "COLUMN_PAGE = \"page\"\n",
    "COLUMN_TEXT = \"text\"\n",
    "COLUMN_LANGUAGE = \"language\"\n",
    "COLUMN_CLEAN_TEXT = \"clean_text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extract text from either a single or multiple research study PDFs. Path is 'studies/papers'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy models once to avoid repeated initialization\n",
    "NLP_MODELS = {\n",
    "    'en': spacy.load(\"en_core_web_lg\"),\n",
    "    'nl': spacy.load(\"nl_core_news_lg\"),\n",
    "    'fr': spacy.load(\"fr_core_news_lg\")\n",
    "}\n",
    "\n",
    "# Ligature replacement dictionary\n",
    "LIGATURES = str.maketrans({\n",
    "    # Standard Latin Ligatures\n",
    "    'ﬁ': 'fi', 'ﬂ': 'fl', 'ﬃ': 'ffi', 'ﬄ': 'ffl', 'ﬀ': 'ff', 'ﬅ': 'ft', 'ﬆ': 'st',\n",
    "    \n",
    "    # Mathematical & Scientific Ligatures\n",
    "    '≤': '<=', '≥': '>=', '≠': '!=', '±': '+-', '→': '->', '∞': 'oo',\n",
    "    '∫': 'int', '∑': 'sum', '∏': 'prod', '∇': 'nabla', '∂': 'partial', '√': 'sqrt'\n",
    "})\n",
    "\n",
    "# Define common OCR fixes\n",
    "OCR_FIXES_PATTERN = re.compile(r'signi\\s*ficant|di\\s*fferent|e\\s*ffective|e\\s*ffect|chil\\s*dren|e\\s*ff\\s*ective|con\\s*fi\\s*dence')\n",
    "\n",
    "# Define punctuation and symbol fixes\n",
    "PUNCTUATION_FIXES = {\n",
    "    '“': '\"', '”': '\"', '‘': \"'\", '’': \"'\",\n",
    "    '—': '-', '–': '-', '…': '...', '•': '*', '·': '*', '●': '*',\n",
    "}\n",
    "\n",
    "# Define normalization fixes for numbers, symbols, and units\n",
    "NORMALIZATION_FIXES = {\n",
    "    '–': '-',  # Replace en dash with hyphen\n",
    "    ' %': '%',  # Remove space before percentage\n",
    "    '^': '',   # Remove caret\n",
    "    'kg.': 'kg',  # Remove period after kg\n",
    "    'C°': '°C',   # Correct temperature symbol\n",
    "}\n",
    "\n",
    "# Define citation and reference fixes\n",
    "CITATION_FIXES = {\n",
    "    'et. al.': 'et al.', \n",
    "    'et al': 'et al.'\n",
    "}\n",
    "\n",
    "# Combine all replacements into a single dictionary\n",
    "ALL_REPLACEMENTS = {**PUNCTUATION_FIXES, **NORMALIZATION_FIXES, **CITATION_FIXES}\n",
    "\n",
    "# Unwanted keywords for filtering\n",
    "UNWANTED_KEYWORDS = frozenset({\n",
    "    'doi', 'https', 'http', 'journal', 'university', 'copyrighted',\n",
    "    'taylor & francis', 'elsevier', 'published by', 'received',\n",
    "    'revised', 'author(s)', 'source:', 'history:', 'keywords',\n",
    "    'volume', 'downloaded', 'article', 'creative commons use',\n",
    "    'authors', 'all rights reserved'\n",
    "})\n",
    "\n",
    "# Reference and Acknowledgement markers\n",
    "REFERENCE_MARKERS = frozenset({'references', 'bibliography', 'acknowledgements', 'method', 'methods'})\n",
    "\n",
    "# Utility functions\n",
    "def validate_pdf_path(pdf_path: str) -> bool:\n",
    "    \"\"\"Checks if the PDF file exists and is readable.\"\"\"\n",
    "    if not os.path.exists(pdf_path) or not os.access(pdf_path, os.R_OK):\n",
    "        logging.error(f\"PDF file is not accessible: {pdf_path}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def is_heading(line: str) -> bool:\n",
    "    \"\"\"Determines if a line is a heading (all uppercase or starts with 'CHAPTER').\"\"\"\n",
    "    return line.isupper() or line.startswith('CHAPTER')\n",
    "\n",
    "def is_footnote(line: str) -> bool:\n",
    "    \"\"\"Identifies footnotes based on common patterns.\"\"\"\n",
    "    return re.match(r'^(\\\\[\\\\d+\\\\]|\\\\d+\\\\.|[*]|Note|Table)', line) is not None\n",
    "\n",
    "def contains_unwanted_keywords(line: str) -> bool:\n",
    "    \"\"\"Checks if the line contains any unwanted keywords.\"\"\"\n",
    "    lower_line = line.lower()\n",
    "    return any(keyword in lower_line for keyword in UNWANTED_KEYWORDS)\n",
    "\n",
    "def is_reference_section(line: str) -> bool:\n",
    "    \"\"\"Checks if the line indicates the start of a reference section.\"\"\"\n",
    "    lower_line = line.lower()\n",
    "    return any(marker in lower_line for marker in REFERENCE_MARKERS)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Cleans text by replacing ligatures, fixing OCR errors, and normalizing symbols.\"\"\"\n",
    "    # Replace ligatures\n",
    "    text = text.translate(LIGATURES)\n",
    "    \n",
    "    # Apply OCR fixes using regex substitution\n",
    "    text = OCR_FIXES_PATTERN.sub(lambda match: ALL_REPLACEMENTS.get(match.group(0), match.group(0)), text)\n",
    "    \n",
    "    # Apply other fixes using a loop for efficiency\n",
    "    for pattern, replacement in ALL_REPLACEMENTS.items():\n",
    "        text = text.replace(pattern, replacement)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def detect_language(text: str) -> str:\n",
    "    \"\"\"Detects the language of a given text block.\"\"\"\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return \"unknown\"\n",
    "\n",
    "def process_pdf(file_path: str, filename: str) -> Generator[List[str], None, None]:\n",
    "    \"\"\"\n",
    "    Processes a PDF file and yields cleaned text data along with metadata.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the PDF file.\n",
    "        filename (str): Name of the PDF file (used for metadata).\n",
    "\n",
    "    Yields:\n",
    "        List[str]: A list containing:\n",
    "            - filename (str): Name of the PDF file.\n",
    "            - page_num (int): Page number where the text was extracted.\n",
    "            - full_text (str): Cleaned and concatenated text from the page.\n",
    "            - language (str): Detected language of the text.\n",
    "    \"\"\"\n",
    "    # Extract the title from the filename (used to filter out irrelevant text)\n",
    "    title = os.path.splitext(filename)[0].lower()\n",
    "\n",
    "    try:\n",
    "        # Open the PDF file using PyMuPDF (fitz)\n",
    "        with fitz.open(file_path) as pdf_document:\n",
    "            # Flag to track if the reference section has been reached\n",
    "            section_reached = False\n",
    "\n",
    "            # Iterate through each page in the PDF\n",
    "            for page_num, page in enumerate(pdf_document, start=1):\n",
    "                # Stop processing if the reference section has been reached\n",
    "                if section_reached:\n",
    "                    break\n",
    "\n",
    "                # Extract text from the page in a structured format (dictionary)\n",
    "                text_dict = page.get_text(\"dict\")\n",
    "\n",
    "                # Iterate through each block of text in the page\n",
    "                for block in text_dict.get(\"blocks\", []):\n",
    "                    # Skip non-text blocks (e.g., images)\n",
    "                    if block.get(\"type\", 1) != 0:\n",
    "                        continue\n",
    "\n",
    "                    # Initialize a list to store lines of the current paragraph\n",
    "                    paragraph = []\n",
    "                    # Track the x-coordinate of the first word in the previous line\n",
    "                    prev_x = None\n",
    "\n",
    "                    # Iterate through each line in the block\n",
    "                    for line in block.get(\"lines\", []):\n",
    "                        # Extract spans (text segments) from the line\n",
    "                        spans = line.get(\"spans\", [])\n",
    "                        # Concatenate text from all spans, replacing semicolons with commas\n",
    "                        line_text = \"\".join(span[\"text\"].replace(';', ',') for span in spans)\n",
    "                        # Clean the text (fix OCR errors, ligatures, etc.)\n",
    "                        line_text = clean_text(line_text)\n",
    "\n",
    "                        # Check if the line indicates the start of a reference section\n",
    "                        if is_reference_section(line_text):\n",
    "                            section_reached = True\n",
    "                            break\n",
    "\n",
    "                        # Skip unwanted lines (headings, footnotes, or lines with unwanted keywords)\n",
    "                        if (is_heading(line_text) or is_footnote(line_text) or\n",
    "                            contains_unwanted_keywords(line_text) or line_text.strip().lower() == title):\n",
    "                            continue\n",
    "\n",
    "                        # Get the x-coordinate of the first word in the current line\n",
    "                        first_word_x = spans[0][\"bbox\"][0] if spans else 0\n",
    "\n",
    "                        # If the x-coordinate is close to the previous line's x-coordinate,\n",
    "                        # treat it as part of the same paragraph\n",
    "                        if prev_x is None or abs(first_word_x - prev_x) < 10:\n",
    "                            paragraph.append(line_text)\n",
    "                        else:\n",
    "                            # If the x-coordinate changes significantly, finalize the current paragraph\n",
    "                            if paragraph:\n",
    "                                full_text = \"\".join(paragraph).strip()\n",
    "                                # Yield the paragraph if it contains enough words\n",
    "                                if len(full_text.split()) >= 10:\n",
    "                                    yield [filename, page_num, full_text, detect_language(full_text)]\n",
    "                            # Start a new paragraph with the current line\n",
    "                            paragraph = [line_text]\n",
    "\n",
    "                        # Update the previous x-coordinate\n",
    "                        prev_x = first_word_x\n",
    "\n",
    "                    # Finalize the last paragraph in the block if the reference section hasn't been reached\n",
    "                    if paragraph and not section_reached:\n",
    "                        full_text = \"\".join(paragraph).strip()\n",
    "                        if len(full_text.split()) >= 10:\n",
    "                            yield [filename, page_num, full_text, detect_language(full_text)]\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log any errors that occur during PDF processing\n",
    "        logging.error(f\"Failed to process {file_path}: {e}\")\n",
    "\n",
    "# Helper function for parallel processing\n",
    "def process_single_pdf(file_path: str, filename: str) -> List[List[str]]:\n",
    "    return list(process_pdf(file_path, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main function that processes the pdf's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "pdf_files = [f for f in os.listdir(PDF_DIRECTORY) if f.endswith('.pdf')]\n",
    "\n",
    "with open(CLEANED_CSV, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"filename\", \"page\", \"text\", \"language\"])\n",
    "\n",
    "    # Process PDFs in parallel\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for filename in pdf_files:\n",
    "            file_path = os.path.join(PDF_DIRECTORY, filename)\n",
    "            if validate_pdf_path(file_path):\n",
    "                futures.append(executor.submit(process_single_pdf, file_path, filename))\n",
    "\n",
    "        # Write results to CSV as they are processed\n",
    "        for future in tqdm(futures, desc=\"Processing PDFs\"):\n",
    "            for row in future.result():\n",
    "                writer.writerow(row)\n",
    "\n",
    "logging.info(f\"Data successfully exported to {CLEANED_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the cleaned DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_cleaned = pd.read_csv(CLEANED_CSV)\n",
    "df_cleaned.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Clean it using spaCy with language support for english, dutch and french.\n",
    "We clean up the text\n",
    "- Remove the name of city, country, geography for better outcome\n",
    "- Remove special characters (only letters)\n",
    "- Convert to lower case\n",
    "- Remove stop words\n",
    "- Remove words of only one or 2 letters ('a', 'I', at,...)\n",
    "- Remove very short sentences\n",
    "- Remove urls \n",
    "- use stemming\n",
    "- remove duplicate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map languages to their respective spaCy models\n",
    "STOPWORDS_MAP: Dict[str, Set[str]] = {\n",
    "    'en': en_stopwords,  # Replace with actual stopwords for English\n",
    "    'fr': fr_stopwords,  # Replace with actual stopwords for French\n",
    "    'nl': nl_stopwords,  # Replace with actual stopwords for Dutch\n",
    "}\n",
    "\n",
    "# Define entity types to remove (Personal Information)\n",
    "PERSONAL_ENTITIES: Set[str] = {\n",
    "    \"PERSON\", \"EMAIL\", \"PHONE\", \"GPE\", \"ORG\", \"NORP\", \"FAC\", \"LOC\", \"PRODUCT\", \n",
    "    \"EVENT\", \"WORK_OF_ART\", \"LAW\", \"DATE\"\n",
    "}\n",
    "\n",
    "# Regex pattern to remove unwanted characters (e.g., emojis, symbols, numbers, etc.)\n",
    "UNWANTED_CHARACTERS_PATTERN = re.compile(\n",
    "    r\"[^\\w\\s\"  # Keep alphanumeric characters and whitespace\n",
    "    r\"ÀÁÂÃÄÅàáâãäå\"  # Allow common accented characters\n",
    "    r\"ÈÉÊËèéêë\" \n",
    "    r\"ÌÍÎÏìíîï\"\n",
    "    r\"ÒÓÔÕÖòóôõö\"\n",
    "    r\"ÙÚÛÜùúûü\"\n",
    "    r\"ÇçÑñ\"  # Allow specific special characters\n",
    "    r\"]\", \n",
    "    flags=re.UNICODE\n",
    ")\n",
    "\n",
    "# Regex pattern to remove numbers\n",
    "NUMBERS_PATTERN = re.compile(r\"\\d+\")\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesses text by removing unwanted characters, numbers, normalizing spaces, and stripping leading/trailing whitespace.\n",
    "    Args:\n",
    "        text (str): Input text to preprocess.\n",
    "    Returns:\n",
    "        str: Preprocessed text.\n",
    "    \"\"\"\n",
    "    # Remove unwanted characters using regex\n",
    "    text = UNWANTED_CHARACTERS_PATTERN.sub(\"\", text)\n",
    "    # Remove numbers using regex\n",
    "    text = NUMBERS_PATTERN.sub(\"\", text)\n",
    "    # Normalize spaces (replace multiple spaces with a single space)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # Strip leading/trailing whitespace\n",
    "    return text.strip()\n",
    "\n",
    "def clean_text_with_spacy(text: str, lang: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans text using spaCy: removes personal entities, stopwords, and lemmatizes words.\n",
    "    Args:\n",
    "        text (str): Input text to clean.\n",
    "        lang (str): Language code (e.g., 'en', 'fr', 'nl').\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    if lang not in NLP_MODELS:\n",
    "        logging.warning(f\"Language model for '{lang}' not found. Defaulting to English.\")\n",
    "        lang = 'en'\n",
    "\n",
    "    nlp = NLP_MODELS.get(lang)\n",
    "    stopwords = STOPWORDS_MAP.get(lang, set())  # Get stopwords for the language, default to empty set\n",
    "\n",
    "    if not nlp:\n",
    "        return text  # If no model is available, return original text\n",
    "\n",
    "    # Preprocess text to remove unwanted characters and numbers\n",
    "    text = preprocess_text(text)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Token processing: lemmatization, stopword removal, personal entity removal\n",
    "    tokens = [\n",
    "        token.lemma_.lower() for token in doc\n",
    "        if token.lemma_  # Ensure lemma exists\n",
    "        and token.ent_type_ not in PERSONAL_ENTITIES  # Remove personal entities\n",
    "        and token.text.lower() not in stopwords  # Remove stopwords\n",
    "        and not token.is_punct  # Remove punctuation\n",
    "        and not token.is_space  # Remove spaces\n",
    "        and len(token.lemma_) > 3  # Remove very short words\n",
    "    ]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def final_clean_csv(input_csv: str):\n",
    "    \"\"\"\n",
    "    Cleans text in a CSV file using spaCy and saves the results.\n",
    "    Args:\n",
    "        input_csv (str): Path to the input CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(input_csv)\n",
    "\n",
    "        # Validate required columns\n",
    "        if COLUMN_TEXT not in df or COLUMN_LANGUAGE not in df:\n",
    "            raise KeyError(f\"CSV must contain '{COLUMN_TEXT}' and '{COLUMN_LANGUAGE}' columns\")\n",
    "\n",
    "        # Apply text cleaning in batches to reduce memory usage\n",
    "        batch_size = 1000  # Adjust based on memory constraints\n",
    "        cleaned_texts = []\n",
    "\n",
    "        for i in range(0, len(df), batch_size):\n",
    "            batch = df.iloc[i:i + batch_size]\n",
    "            cleaned_batch = batch.apply(\n",
    "                lambda row: clean_text_with_spacy(str(row[COLUMN_TEXT]), row[COLUMN_LANGUAGE]),\n",
    "                axis=1\n",
    "            )\n",
    "            cleaned_texts.extend(cleaned_batch)\n",
    "\n",
    "        # Add cleaned text to the DataFrame\n",
    "        df[COLUMN_CLEAN_TEXT] = cleaned_texts\n",
    "\n",
    "        # Save the cleaned CSV\n",
    "        df.to_csv(input_csv, index=False)\n",
    "        logging.info(f\"Cleaned data added to new column in {input_csv}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found: {input_csv}\")\n",
    "    except KeyError as e:\n",
    "        logging.error(f\"Missing required column in CSV: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error processing file: {e}\")\n",
    "\n",
    "# Run the cleaning process\n",
    "final_clean_csv(CLEANED_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 30 rows of the cleaned DataFrame\n",
    "df_final = pd.read_csv(CLEANED_CSV)\n",
    "df_final.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for non-string values to avoid errors before applying topic modeling\n",
    "print(df_final['clean_text'].isnull().sum())  # Count NaN values\n",
    "print(df_final[df_final['clean_text'].apply(lambda x: not isinstance(x, str))])  # Find non-string values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with empty strings to avoid errors\n",
    "df_final['clean_text'] = df_final['clean_text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for non-string values in the 'clean_text' column\n",
    "print(df_final['clean_text'].isnull().sum())  # Count NaN values\n",
    "print(df_final[df_final['clean_text'].apply(lambda x: not isinstance(x, str))])  # Find non-string values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Initialize and fit BERTopic\n",
    "The good thing with BERTopic is that is does most of the work automatically (Meaning, I do not need to bore you to death with details about how it works behind te scenes.)\n",
    "\n",
    "We need to do 3 things\n",
    "1. Initialize BERTopic model\n",
    "2. 'Fit' the model -> this  means: run the model, as you would run a simple linear regression\n",
    "3. Look at the topics via \n",
    "\n",
    "To get started, let's just use the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_filenames_count = df_final['filename'].nunique()\n",
    "print(unique_filenames_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERTopic model\n",
    "topic_model = BERTopic(calculate_probabilities=True, min_topic_size=5, nr_topics=10)\n",
    "\n",
    "# Fit the model with preprocessed text sentences\n",
    "topics, probabilities = topic_model.fit_transform(df_final['clean_text'])\n",
    "\n",
    "# View and inspect topics\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Visualize Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visualize Topic Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(df_final['clean_text'])\n",
    "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = topic_model.get_topic_tree(hierarchical_topics)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Visualize documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_documents(df_final['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add topics and probabilities to the original DataFrame\n",
    "df_final[\"topic_number\"] = np.argmax(probabilities, axis=1)\n",
    "\n",
    "# Also extract the topic names and assign them to the DataFrame\n",
    "info = topic_model.get_topic_info()\n",
    "topic_names = info['Representation']\n",
    "\n",
    "df_final['topic_name'] = df_final['topic_number'].map(topic_names)\n",
    "\n",
    "# Save the updated DataFrame to a CSV\n",
    "\n",
    "df_final['topic_name'] = df_final['topic_number'].map(topic_names)\n",
    "\n",
    "# Save to a new CSV file\n",
    "df_final.to_csv(TOPIC_CSV, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_distribution(probabilities[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Topics per full article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the count of times each topic is chosen within each article\n",
    "article_topic_counts = df_final.groupby('filename')['topic_number'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "# Rename columns to 'Topic X'\n",
    "article_topic_counts.columns = [f'Topic {i}' for i in article_topic_counts.columns]\n",
    "\n",
    "# Display the table\n",
    "print(article_topic_counts)\n",
    "\n",
    "# Plot the distribution for each article\n",
    "article_topic_counts.plot(kind='bar', stacked=True, figsize=(15, 7))\n",
    "plt.title('Topic Distribution per Article (Count)')\n",
    "plt.xlabel('Article')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Topics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the proportion of times each topic is chosen within each article\n",
    "article_topic_proportions = df_final.groupby('filename')['topic_number'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "\n",
    "# Rename columns to 'Topic X'\n",
    "article_topic_proportions.columns = [f'Topic {i}' for i in article_topic_proportions.columns]\n",
    "\n",
    "# Display the table\n",
    "print(article_topic_proportions)\n",
    "\n",
    "# Plot the distribution for each article\n",
    "article_topic_proportions.plot(kind='bar', stacked=True, figsize=(15, 7))\n",
    "plt.title('Topic Distribution per Article (Proportion)')\n",
    "plt.xlabel('Article')\n",
    "plt.ylabel('Proportion')\n",
    "plt.legend(title='Topics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Save the visuals to a pdf file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
